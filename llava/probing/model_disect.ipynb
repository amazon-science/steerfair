{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee50cee-8527-4fa5-9ea5-e311ac18165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-10 21:22:11,685] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1458c5-fcff-4f68-9ecc-35a7fe5522d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baa6e971fde465b912ffa9dc46e4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = 'liuhaotian/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "_, model, image_processor, _ = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259968ca-e15a-450a-9140-0403d2b845b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_name_or_path\": \"liuhaotian/llava-v1.5-13b\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaLlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"freeze_mm_mlp_adapter\": false,\n",
       "  \"freeze_mm_vision_resampler\": false,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 5120,\n",
       "  \"image_aspect_ratio\": \"pad\",\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 13824,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mm_hidden_size\": 1024,\n",
       "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
       "  \"mm_resampler_type\": null,\n",
       "  \"mm_use_im_patch_token\": false,\n",
       "  \"mm_use_im_start_end\": false,\n",
       "  \"mm_vision_select_feature\": \"patch\",\n",
       "  \"mm_vision_select_layer\": -2,\n",
       "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
       "  \"model_type\": \"llava\",\n",
       "  \"num_attention_heads\": 40,\n",
       "  \"num_hidden_layers\": 40,\n",
       "  \"num_key_value_heads\": 40,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"tune_mm_mlp_adapter\": false,\n",
       "  \"tune_mm_vision_resampler\": false,\n",
       "  \"unfreeze_mm_vision_tower\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_mm_proj\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daf9b36-1161-49cb-9136-4f72897d6e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionConfig {\n",
       "  \"_name_or_path\": \"openai/clip-vit-large-patch14-336\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"dropout\": 0.0,\n",
       "  \"hidden_act\": \"quick_gelu\",\n",
       "  \"hidden_size\": 1024,\n",
       "  \"image_size\": 336,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"model_type\": \"clip_vision_model\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"patch_size\": 14,\n",
       "  \"projection_dim\": 768,\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.vision_tower.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39d6442-57e3-49a4-8197-753c93f3aeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.self_attn.rotary_emb\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.act_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.self_attn.rotary_emb\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.act_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.self_attn.rotary_emb\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.act_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.self_attn.rotary_emb\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.act_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.self_attn.rotary_emb\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.act_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.self_attn.rotary_emb\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.act_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.self_attn.rotary_emb\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.act_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.self_attn.rotary_emb\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.act_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.self_attn.rotary_emb\n",
      "model.layers.30.mlp\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.mlp.act_fn\n",
      "model.layers.30.input_layernorm\n",
      "model.layers.30.post_attention_layernorm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.self_attn.rotary_emb\n",
      "model.layers.31.mlp\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.mlp.act_fn\n",
      "model.layers.31.input_layernorm\n",
      "model.layers.31.post_attention_layernorm\n",
      "model.layers.32\n",
      "model.layers.32.self_attn\n",
      "model.layers.32.self_attn.q_proj\n",
      "model.layers.32.self_attn.k_proj\n",
      "model.layers.32.self_attn.v_proj\n",
      "model.layers.32.self_attn.o_proj\n",
      "model.layers.32.self_attn.rotary_emb\n",
      "model.layers.32.mlp\n",
      "model.layers.32.mlp.gate_proj\n",
      "model.layers.32.mlp.up_proj\n",
      "model.layers.32.mlp.down_proj\n",
      "model.layers.32.mlp.act_fn\n",
      "model.layers.32.input_layernorm\n",
      "model.layers.32.post_attention_layernorm\n",
      "model.layers.33\n",
      "model.layers.33.self_attn\n",
      "model.layers.33.self_attn.q_proj\n",
      "model.layers.33.self_attn.k_proj\n",
      "model.layers.33.self_attn.v_proj\n",
      "model.layers.33.self_attn.o_proj\n",
      "model.layers.33.self_attn.rotary_emb\n",
      "model.layers.33.mlp\n",
      "model.layers.33.mlp.gate_proj\n",
      "model.layers.33.mlp.up_proj\n",
      "model.layers.33.mlp.down_proj\n",
      "model.layers.33.mlp.act_fn\n",
      "model.layers.33.input_layernorm\n",
      "model.layers.33.post_attention_layernorm\n",
      "model.layers.34\n",
      "model.layers.34.self_attn\n",
      "model.layers.34.self_attn.q_proj\n",
      "model.layers.34.self_attn.k_proj\n",
      "model.layers.34.self_attn.v_proj\n",
      "model.layers.34.self_attn.o_proj\n",
      "model.layers.34.self_attn.rotary_emb\n",
      "model.layers.34.mlp\n",
      "model.layers.34.mlp.gate_proj\n",
      "model.layers.34.mlp.up_proj\n",
      "model.layers.34.mlp.down_proj\n",
      "model.layers.34.mlp.act_fn\n",
      "model.layers.34.input_layernorm\n",
      "model.layers.34.post_attention_layernorm\n",
      "model.layers.35\n",
      "model.layers.35.self_attn\n",
      "model.layers.35.self_attn.q_proj\n",
      "model.layers.35.self_attn.k_proj\n",
      "model.layers.35.self_attn.v_proj\n",
      "model.layers.35.self_attn.o_proj\n",
      "model.layers.35.self_attn.rotary_emb\n",
      "model.layers.35.mlp\n",
      "model.layers.35.mlp.gate_proj\n",
      "model.layers.35.mlp.up_proj\n",
      "model.layers.35.mlp.down_proj\n",
      "model.layers.35.mlp.act_fn\n",
      "model.layers.35.input_layernorm\n",
      "model.layers.35.post_attention_layernorm\n",
      "model.layers.36\n",
      "model.layers.36.self_attn\n",
      "model.layers.36.self_attn.q_proj\n",
      "model.layers.36.self_attn.k_proj\n",
      "model.layers.36.self_attn.v_proj\n",
      "model.layers.36.self_attn.o_proj\n",
      "model.layers.36.self_attn.rotary_emb\n",
      "model.layers.36.mlp\n",
      "model.layers.36.mlp.gate_proj\n",
      "model.layers.36.mlp.up_proj\n",
      "model.layers.36.mlp.down_proj\n",
      "model.layers.36.mlp.act_fn\n",
      "model.layers.36.input_layernorm\n",
      "model.layers.36.post_attention_layernorm\n",
      "model.layers.37\n",
      "model.layers.37.self_attn\n",
      "model.layers.37.self_attn.q_proj\n",
      "model.layers.37.self_attn.k_proj\n",
      "model.layers.37.self_attn.v_proj\n",
      "model.layers.37.self_attn.o_proj\n",
      "model.layers.37.self_attn.rotary_emb\n",
      "model.layers.37.mlp\n",
      "model.layers.37.mlp.gate_proj\n",
      "model.layers.37.mlp.up_proj\n",
      "model.layers.37.mlp.down_proj\n",
      "model.layers.37.mlp.act_fn\n",
      "model.layers.37.input_layernorm\n",
      "model.layers.37.post_attention_layernorm\n",
      "model.layers.38\n",
      "model.layers.38.self_attn\n",
      "model.layers.38.self_attn.q_proj\n",
      "model.layers.38.self_attn.k_proj\n",
      "model.layers.38.self_attn.v_proj\n",
      "model.layers.38.self_attn.o_proj\n",
      "model.layers.38.self_attn.rotary_emb\n",
      "model.layers.38.mlp\n",
      "model.layers.38.mlp.gate_proj\n",
      "model.layers.38.mlp.up_proj\n",
      "model.layers.38.mlp.down_proj\n",
      "model.layers.38.mlp.act_fn\n",
      "model.layers.38.input_layernorm\n",
      "model.layers.38.post_attention_layernorm\n",
      "model.layers.39\n",
      "model.layers.39.self_attn\n",
      "model.layers.39.self_attn.q_proj\n",
      "model.layers.39.self_attn.k_proj\n",
      "model.layers.39.self_attn.v_proj\n",
      "model.layers.39.self_attn.o_proj\n",
      "model.layers.39.self_attn.rotary_emb\n",
      "model.layers.39.mlp\n",
      "model.layers.39.mlp.gate_proj\n",
      "model.layers.39.mlp.up_proj\n",
      "model.layers.39.mlp.down_proj\n",
      "model.layers.39.mlp.act_fn\n",
      "model.layers.39.input_layernorm\n",
      "model.layers.39.post_attention_layernorm\n",
      "model.norm\n",
      "model.vision_tower\n",
      "model.vision_tower.vision_tower\n",
      "model.vision_tower.vision_tower.vision_model\n",
      "model.vision_tower.vision_tower.vision_model.embeddings\n",
      "model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding\n",
      "model.vision_tower.vision_tower.vision_model.embeddings.position_embedding\n",
      "model.vision_tower.vision_tower.vision_model.pre_layrnorm\n",
      "model.vision_tower.vision_tower.vision_model.encoder\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.activation_fn\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2\n",
      "model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2\n",
      "model.vision_tower.vision_tower.vision_model.post_layernorm\n",
      "model.mm_projector\n",
      "model.mm_projector.0\n",
      "model.mm_projector.1\n",
      "model.mm_projector.2\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_modules():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49bc4b4b-00ea-400f-94c5-b587d5ce9d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_attn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(k, \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n",
      "File \u001b[0;32m/opt/conda/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_modules():\n",
    "    if k.split('.')[-1]=='self_attn' in k:\n",
    "        print(k, v.o_proj.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3570a-961a-4ca3-a245-e255201748e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_modules():\n",
    "     if 'o_proj' in k:\n",
    "        print('key', k)\n",
    "        print('value', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2124b-31ca-4e81-95b6-2e479ff38996",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model.model.vision_tower.named_modules():\n",
    "    if 'out_proj' in k:\n",
    "        print(k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6ddff-b02a-4655-9abd-b058dfbecb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
